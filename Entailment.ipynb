{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project aims to take input as paragraphs/text and extract problem statements from junk.\n",
    "The logic followed is that problem statements , though scattered , together create one meaningful text slab.\n",
    "We beleive the problem statement to have negative inclination and hence we use sentiment analysis to segregate the negative \n",
    "groups.\n",
    "\n",
    "We use ALLENNLP TEXT ENTAILMENT to group together the problem Statements.\n",
    "We then treat group elements \n",
    "\n",
    "We then Use ALLENNLP Sentiment Analysis to label the groups as postive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import wait, ALL_COMPLETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_jsonnet not loaded, treating C:\\Users\\lsethia\\AppData\\Local\\Temp\\tmpbqfz7n9n\\config.json as json\n",
      "_jsonnet not loaded, treating snippet as json\n",
      "Did not use initialization regex that was passed: .*token_embedder_tokens\\._projection.*weight\n",
      "_jsonnet not loaded, treating C:\\Users\\lsethia\\AppData\\Local\\Temp\\tmpd44ilup4\\config.json as json\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/decomposable-attention-elmo-2018.02.19.tar.gz\")\n",
    "predictor_s = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/sst-2-basic-classifier-glove-2019.06.27.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(r\"train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5367, 15)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text=str(text).lower().split(\"\\n\") \n",
    "    clean=[]\n",
    "    for i in text:\n",
    "        temp=\"\".join(a for a in i if a.isalnum() or a==\" \")\n",
    "#         temp=re.sub(r'\\w*\\d\\w*', '',temp).strip()\n",
    "        clean.append(temp)\n",
    "    clean=set(clean)\n",
    "    cleant=list(clean)\n",
    "    clean_text=\"\\n\".join(i for i in clean)\n",
    "    return clean_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data['Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is  1\n",
      "['check for oem blackout']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a mapping for the data so that we can insert the data into the correct postion during parallel processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped=[]\n",
    "for i in range(len(X)):\n",
    "    mapped.append([i,X[i]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here n denotes the number of entires to be parsed into groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=300\n",
    "groups=[0]*n\n",
    "def entail(inp):\n",
    "    i=inp[1]\n",
    "    i=re.sub(\" +\",\" \",i)\n",
    "    reso_g=[]\n",
    "    temp=i.split(\"\\n\")\n",
    "    temp=[a for a in temp if len(a)>1]\n",
    "    length=len(temp)\n",
    "    print(\"length is \",length)\n",
    "    print(temp)\n",
    "    if(length==1):\n",
    "        groups[inp[0]]=temp\n",
    "    else:       \n",
    "        for j in range(len(temp)):\n",
    "            if(temp[j]!=\"-1\"):                \n",
    "                group_temp=[temp[j]]\n",
    "                for k in range(j+1,len(temp)):\n",
    "                    if(temp[k]!=\"-1\"):\n",
    "                        score=predictor.predict(hypothesis=temp[j],premise=temp[k])\n",
    "                        if(score[\"label_probs\"][0]>0.65):\n",
    "                            group_temp.append(temp[k])\n",
    "                            temp[k]=\"-1\"\n",
    "                reso_g.append(group_temp)               \n",
    "            else:\n",
    "                continue\n",
    "#         groups.append([inp[0],reso_g])\n",
    "        groups[inp[0]]=reso_g\n",
    "    return \"Done\"\n",
    "            \n",
    "                \n",
    "#         while(len(temp)>0):\n",
    "#             k=0\n",
    "#             while(k<len(temp)):\n",
    "                \n",
    "#             for k in range(j,len(temp)):            \n",
    "#             score=predictor.predict(hypothesis=temp[j],\n",
    "#               premise=temp[k]\n",
    "#             )\n",
    "#             if(score[\"label_probs\"][0]>0.65):\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    future = executor.map(entail, mapped[:n])\n",
    "    executor.shutdown(wait=True)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join group data into a single sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_group=[]\n",
    "for i in groups:\n",
    "    gro=[]\n",
    "    if(type(i) is not int):\n",
    "        for j in i:\n",
    "            temp=\" \".join(f for f in j )\n",
    "            gro.append(temp)\n",
    "        join_group.append(gro)\n",
    "    else:\n",
    "        join_group.append([str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment=[]\n",
    "for i in join_group:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        \n",
    "        f=predictor_s.predict(\n",
    "  sentence=j\n",
    ")\n",
    "        if(f['label']=='0'):\n",
    "            temp.append(f)\n",
    "        else:\n",
    "            temp.append(f)\n",
    "    sentiment.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'logits': [-3.6275806427001953, 3.6046507358551025],\n",
       "  'probs': [0.0007223839638754725, 0.999277651309967],\n",
       "  'label': '0'},\n",
       " {'logits': [-2.507786750793457, 2.4919331073760986],\n",
       "  'probs': [0.006694715470075607, 0.9933052659034729],\n",
       "  'label': '0'},\n",
       " {'logits': [1.5602668523788452, -1.4158766269683838],\n",
       "  'probs': [0.9514846205711365, 0.04851534590125084],\n",
       "  'label': '1'},\n",
       " {'logits': [1.6364237070083618, -1.562311053276062],\n",
       "  'probs': [0.9607866406440735, 0.03921336308121681],\n",
       "  'label': '1'},\n",
       " {'logits': [-3.457111358642578, 3.5013718605041504],\n",
       "  'probs': [0.0009496345301158726, 0.9990503191947937],\n",
       "  'label': '0'},\n",
       " {'logits': [-1.28526771068573, 1.3379082679748535],\n",
       "  'probs': [0.06766165792942047, 0.9323383569717407],\n",
       "  'label': '0'},\n",
       " {'logits': [-2.947356700897217, 3.0252346992492676],\n",
       "  'probs': [0.0025411571841686964, 0.9974588751792908],\n",
       "  'label': '0'},\n",
       " {'logits': [3.376458168029785, -3.2564785480499268],\n",
       "  'probs': [0.9986854195594788, 0.001314561697654426],\n",
       "  'label': '1'}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(join_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp={\n",
    "    'description':mapped[:n],\n",
    "    'groups':groups,\n",
    "    'sentiments':sentiment\n",
    "}\n",
    "inp=pd.DataFrame(inp)\n",
    "inp.to_excel(\"TExt entailment + Sentiment analysis.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
